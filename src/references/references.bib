% Encoding: UTF-8
@article{Wongsuphasawat2018,
   abstract = {We present a design study of the TensorFlow Graph Visualizer, part of the TensorFlow machine intelligence platform. This tool helps users understand complex machine learning architectures by visualizing their underlying dataflow graphs. The tool works by applying a series of graph transformations that enable standard layout techniques to produce a legible interactive diagram. To declutter the graph, we decouple non-critical nodes from the layout. To provide an overview, we build a clustered graph using the hierarchical structure annotated in the source code. To support exploration of nested structure on demand, we perform edge bundling to enable stable and responsive cluster expansion. Finally, we detect and highlight repeated structures to emphasize a model's modular composition. To demonstrate the utility of the visualizer, we describe example usage scenarios and report user feedback. Overall, users find the visualizer useful for understanding, debugging, and sharing the structures of their models.},
   author = {Kanit Wongsuphasawat and Daniel Smilkov and James Wexler and Jimbo Wilson and Dandelion Mané and Doug Fritz and Dilip Krishnan and Fernanda B. Viégas and Martin Wattenberg},
   doi = {10.1109/TVCG.2017.2744878},
   issn = {10772626},
   issue = {1},
   journal = {IEEE Transactions on Visualization and Computer Graphics},
   title = {{Visualizing Dataflow Graphs of Deep Learning Models in TensorFlow}},
   volume = {24},
   year = {2018},
}
@article{Xu2017,
   abstract = {Visual analytics plays a key role in the era of connected industry (or industry 4.0, industrial internet) as modern machines and assembly lines generate large amounts of data and effective visual exploration techniques are needed for troubleshooting, process optimization, and decision making. However, developing effective visual analytics solutions for this application domain is a challenging task due to the sheer volume and the complexity of the data collected in the manufacturing processes. We report the design and implementation of a comprehensive visual analytics system, ViDX. It supports both real-time tracking of assembly line performance and historical data exploration to identify inefficiencies, locate anomalies, and form hypotheses about their causes and effects. The system is designed based on a set of requirements gathered through discussions with the managers and operators from manufacturing sites. It features interlinked views displaying data at different levels of detail. In particular, we apply and extend the Marey's graph by introducing a time-aware outlier-preserving visual aggregation technique to support effective troubleshooting in manufacturing processes. We also introduce two novel interaction techniques, namely the quantiles brush and samples brush, for the users to interactively steer the outlier detection algorithms. We evaluate the system with example use cases and an in-depth user interview, both conducted together with the managers and operators from manufacturing plants. The result demonstrates its effectiveness and reports a successful pilot application of visual analytics for manufacturing in smart factories.},
   author = {Panpan Xu and Honghui Mei and Liu Ren and Wei Chen},
   doi = {10.1109/TVCG.2016.2598664},
   issn = {10772626},
   issue = {1},
   journal = {IEEE Transactions on Visualization and Computer Graphics},
   title = {{ViDX: Visual Diagnostics of Assembly Line Performance in Smart Factories}},
   volume = {23},
   year = {2017},
}
@article{Blascheck2016,
   abstract = {Evaluation has become a fundamental part of visualization research and researchers have employed many approaches from the field of human-computer interaction like measures of task performance, thinking aloud protocols, and analysis of interaction logs. Recently, eye tracking has also become popular to analyze visual strategies of users in this context. This has added another modality and more data, which requires special visualization techniques to analyze this data. However, only few approaches exist that aim at an integrated analysis of multiple concurrent evaluation procedures. The variety, complexity, and sheer amount of such coupled multi-source data streams require a visual analytics approach. Our approach provides a highly interactive visualization environment to display and analyze thinking aloud, interaction, and eye movement data in close relation. Automatic pattern finding algorithms allow an efficient exploratory search and support the reasoning process to derive common eye-interaction-thinking patterns between participants. In addition, our tool equips researchers with mechanisms for searching and verifying expected usage patterns. We apply our approach to a user study involving a visual analytics application and we discuss insights gained from this joint analysis. We anticipate our approach to be applicable to other combinations of evaluation techniques and a broad class of visualization applications.},
   author = {Tanja Blascheck and Markus John and Kuno Kurzhals and Steffen Koch and Thomas Ertl},
   doi = {10.1109/TVCG.2015.2467871},
   issn = {10772626},
   issue = {1},
   journal = {IEEE Transactions on Visualization and Computer Graphics},
   title = {{VA2: A Visual Analytics Approach for Evaluating Visual Analytics Applications}},
   volume = {22},
   year = {2016},
}
@article{Unger2018,
   abstract = {This design study focuses on the analysis of a time sequence of categorical sequences. Such data is relevant for the geoscientific research field of landscape and climate development. It results from microscopic analysis of lake sediment cores. The goal is to gain hypotheses about landscape evolution and climate conditions in the past. To this end, geoscientists identify which categorical sequences are similar in the sense that they indicate similar conditions. Categorical sequences are similar if they have similar meaning (semantic similarity) and appear in similar time periods (temporal similarity). For data sets with many different categorical sequences, the task to identify similar sequences becomes a challenge. Our contribution is a tailored visual analysis concept that effectively supports the analytical process. Our visual interface comprises coupled visualizations of semantics and temporal context for the exploration and assessment of the similarity of categorical sequences. Integrated automatic methods reduce the analytical effort substantially. They (1) extract unique sequences in the data and (2) rank sequences by a similarity measure during the search for similar sequences. We evaluated our concept by demonstrations of our prototype to a larger audience and hands-on analysis sessions for two different lakes. According to geoscientists, our approach fills an important methodological gap in the application domain.},
   author = {Andrea Unger and Nadine Dräger and Mike Sips and Dirk J. Lehmann},
   doi = {10.1109/TVCG.2017.2744686},
   issn = {10772626},
   issue = {1},
   journal = {IEEE Transactions on Visualization and Computer Graphics},
   title = {{Understanding a Sequence of Sequences: Visual Exploration of Categorical States in Lake Sediment Cores}},
   volume = {24},
   year = {2018},
}
@article{Liu2019,
   abstract = {Consider a multi-dimensional spatio-temporal (ST) dataset where each entry is a numerical measure defined by the corresponding temporal, spatial and other domain-specific dimensions. A typical approach to explore such data utilizes interactive visualizations with multiple coordinated views. Each view displays the aggregated measures along one or two dimensions. By brushing on the views, analysts can obtain detailed information. However, this approach often cannot provide sufficient guidance for analysts to identify patterns hidden within subsets of data. Without a priori hypotheses, analysts need to manually select and iterate through different slices to search for patterns, which can be a tedious and lengthy process. In this work, we model multidimensional ST data as tensors and propose a novel piecewise rank-one tensor decomposition algorithm which supports automatically slicing the data into homogeneous partitions and extracting the latent patterns in each partition for comparison and visual summarization. The algorithm optimizes a quantitative measure about how faithfully the extracted patterns visually represent the original data. Based on the algorithm we further propose a visual analytics framework that supports a top-down, progressive partitioning workflow for level-of-detail multidimensional ST data exploration. We demonstrate the general applicability and effectiveness of our technique on three datasets from different application domains: regional sales trend analysis, customer traffic analysis in department stores, and taxi trip analysis with origin-destination (OD) data. We further interview domain experts to verify the usability of the prototype.},
   author = {Dongyu Liu and Panpan Xu and Liu Ren},
   doi = {10.1109/TVCG.2018.2865018},
   issn = {19410506},
   issue = {1},
   journal = {IEEE Transactions on Visualization and Computer Graphics},
   title = {{TPFlow: Progressive Partition and Multidimensional Pattern Extraction for Large-Scale Spatio-Temporal Data Analysis}},
   volume = {25},
   year = {2019},
}
@article{Niederer2018,
   abstract = {Multivariate, tabular data is one of the most common data structures used in many different domains. Over time, tables can undergo changes in both structure and content, which results in multiple versions of the same table. A challenging task when working with such derived tables is to understand what exactly has changed between versions in terms of additions/deletions, reorder, merge/split, and content changes. For textual data, a variety of commonplace 'diff' tools exist that support the task of investigating changes between revisions of a text. Although there are some comparison tools which assist users in inspecting differences between multiple table instances, the resulting visualizations are often difficult to interpret or do not scale to large tables with thousands of rows and columns. To address these challenges, we developed TACO, an interactive comparison tool that visualizes the differences between multiple tables at various levels of detail. With TACO we show (1) the aggregated differences between multiple table versions over time, (2) the aggregated changes between two selected table versions, and (3) detailed changes between the selected tables. To demonstrate the effectiveness of our approach, we show its application by means of two usage scenarios.},
   author = {Christina Niederer and Holger Stitz and Reem Hourieh and Florian Grassinger and Wolfgang Aigner and Marc Streit},
   doi = {10.1109/TVCG.2017.2745298},
   issn = {10772626},
   issue = {1},
   journal = {IEEE Transactions on Visualization and Computer Graphics},
   title = {{TACO: Visualizing Changes in Tables over Time}},
   volume = {24},
   year = {2018},
}
@article{Fujiwara2020,
   abstract = {Dimensionality reduction (DR) is frequently used for analyzing and visualizing high-dimensional data as it provides a good first glance of the data. However, to interpret the DR result for gaining useful insights from the data, it would take additional analysis effort such as identifying clusters and understanding their characteristics. While there are many automatic methods (e.g., density-based clustering methods) to identify clusters, effective methods for understanding a cluster's characteristics are still lacking. A cluster can be mostly characterized by its distribution of feature values. Reviewing the original feature values is not a straightforward task when the number of features is large. To address this challenge, we present a visual analytics method that effectively highlights the essential features of a cluster in a DR result. To extract the essential features, we introduce an enhanced usage of contrastive principal component analysis (cPCA). Our method, called ccPCA (contrasting clusters in PCA), can calculate each feature's relative contribution to the contrast between one cluster and other clusters. With ccPCA, we have created an interactive system including a scalable visualization of clusters' feature contributions. We demonstrate the effectiveness of our method and system with case studies using several publicly available datasets.},
   author = {Takanori Fujiwara and Oh Hyun Kwon and Kwan Liu Ma},
   doi = {10.1109/TVCG.2019.2934251},
   issn = {19410506},
   issue = {1},
   journal = {IEEE Transactions on Visualization and Computer Graphics},
   title = {{Supporting Analysis of Dimensionality Reduction Results with Contrastive Learning}},
   volume = {26},
   year = {2020},
}
@article{Liu2017,
   abstract = {The problem of formulating solutions immediately and comparing them rapidly for billboard placements has plagued advertising planners for a long time, owing to the lack of efficient tools for in-depth analyses to make informed decisions. In this study, we attempt to employ visual analytics that combines the state-of-the-art mining and visualization techniques to tackle this problem using large-scale GPS trajectory data. In particular, we present SmartAdP, an interactive visual analytics system that deals with the two major challenges including finding good solutions in a huge solution space and comparing the solutions in a visual and intuitive manner. An interactive framework that integrates a novel visualization-driven data mining model enables advertising planners to effectively and efficiently formulate good candidate solutions. In addition, we propose a set of coupled visualizations: a solution view with metaphor-based glyphs to visualize the correlation between different solutions; a location view to display billboard locations in a compact manner; and a ranking view to present multi-typed rankings of the solutions. This system has been demonstrated using case studies with a real-world dataset and domain-expert interviews. Our approach can be adapted for other location selection problems such as selecting locations of retail stores or restaurants using trajectory data.},
   author = {Dongyu Liu and Di Weng and Yuhong Li and Jie Bao and Yu Zheng and Huamin Qu and Yingcai Wu},
   doi = {10.1109/TVCG.2016.2598432},
   issn = {10772626},
   issue = {1},
   journal = {IEEE Transactions on Visualization and Computer Graphics},
   title = {{SmartAdP: Visual Analytics of Large-scale Taxi Trajectories for Selecting Billboard Locations}},
   volume = {23},
   year = {2017},
}
@article{Strobelt2019,
   abstract = {Neural sequence-to-sequence models have proven to be accurate and robust for many sequence prediction tasks, and have become the standard approach for automatic translation of text. The models work with a five-stage blackbox pipeline that begins with encoding a source sequence to a vector space and then decoding out to a new target sequence. This process is now standard, but like many deep learning methods remains quite difficult to understand or debug. In this work, we present a visual analysis tool that allows interaction and 'what if'-style exploration of trained sequence-to-sequence models through each stage of the translation process. The aim is to identify which patterns have been learned, to detect model errors, and to probe the model with counterfactual scenario. We demonstrate the utility of our tool through several real-world sequence-to-sequence use cases on large-scale models.},
   author = {Hendrik Strobelt and Sebastian Gehrmann and Michael Behrisch and Adam Perer and Hanspeter Pfister and Alexander M. Rush},
   doi = {10.1109/TVCG.2018.2865044},
   issn = {19410506},
   issue = {1},
   journal = {IEEE Transactions on Visualization and Computer Graphics},
   title = {{Seq2seq-Vis: A Visual Debugging Tool for Sequence-to-Sequence Models}},
   volume = {25},
   year = {2019},
}
@article{Borland2020,
   abstract = {The collection of large, complex datasets has become common across a wide variety of domains. Visual analytics tools increasingly play a key role in exploring and answering complex questions about these large datasets. However, many visualizations are not designed to concurrently visualize the large number of dimensions present in complex datasets (e.g.Tens of thousands of distinct codes in an electronic health record system). This fact, combined with the ability of many visual analytics systems to enable rapid, ad-hoc specification of groups, or cohorts, of individuals based on a small subset of visualized dimensions, leads to the possibility of introducing selection bias-when the user creates a cohort based on a specified set of dimensions, differences across many other unseen dimensions may also be introduced. These unintended side effects may result in the cohort no longer being representative of the larger population intended to be studied, which can negatively affect the validity of subsequent analyses. We present techniques for selection bias tracking and visualization that can be incorporated into high-dimensional exploratory visual analytics systems, with a focus on medical data with existing data hierarchies. These techniques include: (1) tree-based cohort provenance and visualization, including a user-specified baseline cohort that all other cohorts are compared against, and visual encoding of cohort 'drift', which indicates where selection bias may have occurred, and (2) a set of visualizations, including a novel icicle-plot based visualization, to compare in detail the per-dimension differences between the baseline and a user-specified focus cohort. These techniques are integrated into a medical temporal event sequence visual analytics tool. We present example use cases and report findings from domain expert user interviews.},
   author = {David Borland and Wenyuan Wang and Jonathan Zhang and Joshua Shrestha and David Gotz},
   doi = {10.1109/TVCG.2019.2934209},
   issn = {19410506},
   issue = {1},
   journal = {IEEE Transactions on Visualization and Computer Graphics},
   title = {{Selection Bias Tracking and Detailed Subset Comparison for High-Dimensional Data}},
   volume = {26},
   year = {2020},
}
@article{Hoque2020,
   abstract = {We present a search engine for D3 visualizations that allows queries based on their visual style and underlying structure. To build the engine we crawl a collection of 7860 D3 visualizations from the Web and deconstruct each one to recover its data, its data-encoding marks and the encodings describing how the data is mapped to visual attributes of the marks. We also extract axes and other non-data-encoding attributes of marks (e.g., typeface, background color). Our search engine indexes this style and structure information as well as metadata about the webpage containing the chart. We show how visualization developers can search the collection to find visualizations that exhibit specific design characteristics and thereby explore the space of possible designs. We also demonstrate how researchers can use the search engine to identify commonly used visual design patterns and we perform such a demographic design analysis across our collection of D3 charts. A user study reveals that visualization developers found our style and structure based search engine to be significantly more useful and satisfying for finding different designs of D3 charts, than a baseline search engine that only allows keyword search over the webpage containing a chart.},
   author = {Enamul Hoque and Maneesh Agrawala},
   doi = {10.1109/TVCG.2019.2934431},
   issn = {19410506},
   issue = {1},
   journal = {IEEE Transactions on Visualization and Computer Graphics},
   title = {{Searching the Visual Style and Structure of D3 Visualizations}},
   volume = {26},
   year = {2020},
}
@article{Kwon2019,
   abstract = {We have recently seen many successful applications of recurrent neural networks (RNNs) on electronic medical records (EMRs), which contain histories of patients' diagnoses, medications, and other various events, in order to predict the current and future states of patients. Despite the strong performance of RNNs, it is often challenging for users to understand why the model makes a particular prediction. Such black-box nature of RNNs can impede its wide adoption in clinical practice. Furthermore, we have no established methods to interactively leverage users' domain expertise and prior knowledge as inputs for steering the model. Therefore, our design study aims to provide a visual analytics solution to increase interpretability and interactivity of RNNs via a joint effort of medical experts, artificial intelligence scientists, and visual analytics researchers. Following the iterative design process between the experts, we design, implement, and evaluate a visual analytics tool called RetainVis, which couples a newly improved, interpretable, and interactive RNN-based model called RetainEX and visualizations for users' exploration of EMR data in the context of prediction tasks. Our study shows the effective use of RetainVis for gaining insights into how individual medical codes contribute to making risk predictions, using EMRs of patients with heart failure and cataract symptoms. Our study also demonstrates how we made substantial changes to the state-of-the-art RNN model called RETAIN in order to make use of temporal information and increase interactivity. This study will provide a useful guideline for researchers that aim to design an interpretable and interactive visual analytics tool for RNNs.},
   author = {Bum Chul Kwon and Min Je Choi and Joanne Taery Kim and Edward Choi and Young Bin Kim and Soonwook Kwon and Jimeng Sun and Jaegul Choo},
   doi = {10.1109/TVCG.2018.2865027},
   issn = {19410506},
   issue = {1},
   journal = {IEEE Transactions on Visualization and Computer Graphics},
   title = {{RetainVis: Visual Analytics with Interpretable and Interactive Recurrent Neural Networks on Electronic Medical Records}},
   volume = {25},
   year = {2019},
}

@Article{Elzen2016,
  author   = {Stef Van Den Elzen and Danny Holten and Jorik Blaas and Jarke J. Van Wijk},
  title = {{Reducing Snapshots to Points: A Visual Analytics Approach to Dynamic Network Exploration}},
  journal  = {IEEE Transactions on Visualization and Computer Graphics},
  year     = {2016},
  volume   = {22},
  issn     = {10772626},
  abstract = {We propose a visual analytics approach for the exploration and analysis of dynamic networks. We consider snapshots of the network as points in high-dimensional space and project these to two dimensions for visualization and interaction using two juxtaposed views: one for showing a snapshot and one for showing the evolution of the network. With this approach users are enabled to detect stable states, recurring states, outlier topologies, and gain knowledge about the transitions between states and the network evolution in general. The components of our approach are discretization, vectorization and normalization, dimensionality reduction, and visualization and interaction, which are discussed in detail. The effectiveness of the approach is shown by applying it to artificial and real-world dynamic networks.},
  doi      = {10.1109/TVCG.2015.2468078},
  issue    = {1},
}
@article{Shi2012,
   abstract = {For many applications involving time series data, people are often interested in the changes of item values over time as well as their ranking changes. For example, people search many words via search engines like Google and Bing every day. Analysts are interested in both the absolute searching number for each word as well as their relative rankings. Both sets of statistics may change over time. For very large time series data with thousands of items, how to visually present ranking changes is an interesting challenge. In this paper, we propose RankExplorer, a novel visualization method based on ThemeRiver to reveal the ranking changes. Our method consists of four major components: 1) a segmentation method which partitions a large set of time series curves into a manageable number of ranking categories; 2) an extended ThemeRiver view with embedded color bars and changing glyphs to show the evolution of aggregation values related to each ranking category over time as well as the content changes in each ranking category; 3) a trend curve to show the degree of ranking changes over time; 4) rich user interactions to support interactive exploration of ranking changes. We have applied our method to some real time series data and the case studies demonstrate that our method can reveal the underlying patterns related to ranking changes which might otherwise be obscured in traditional visualizations. © 1995-2012 IEEE.},
   author = {Conglei Shi and Weiwei Cui and Shixia Liu and Panpan Xu and Wei Chen and Huamin Qu},
   doi = {10.1109/TVCG.2012.253},
   issn = {10772626},
   issue = {12},
   journal = {IEEE Transactions on Visualization and Computer Graphics},
   title = {{Rankexplorer: Visualization of Ranking Changes in Large Time Series Data}},
   volume = {18},
   year = {2012},
}

@Article{El-Assady2018,
  author   = {Mennatallah El-Assady and Rita Sevastjanova and Fabian Sperrle and Daniel Keim and Christopher Collins},
  title = {{Progressive Learning of Topic Modeling Parameters: A Visual Analytics Framework}},
  journal  = {IEEE Transactions on Visualization and Computer Graphics},
  year     = {2018},
  volume   = {24},
  issn     = {10772626},
  abstract = {Topic modeling algorithms are widely used to analyze the thematic composition of text corpora but remain difficult to interpret and adjust. Addressing these limitations, we present a modular visual analytics framework, tackling the understandability and adaptability of topic models through a user-driven reinforcement learning process which does not require a deep understanding of the underlying topic modeling algorithms. Given a document corpus, our approach initializes two algorithm configurations based on a parameter space analysis that enhances document separability. We abstract the model complexity in an interactive visual workspace for exploring the automatic matching results of two models, investigating topic summaries, analyzing parameter distributions, and reviewing documents. The main contribution of our work is an iterative decision-making technique in which users provide a document-based relevance feedback that allows the framework to converge to a user-endorsed topic distribution. We also report feedback from a two-stage study which shows that our technique results in topic model quality improvements on two independent measures.},
  doi      = {10.1109/TVCG.2017.2745080},
  issue    = {1},
}
@article{Wall2018,
   abstract = {People often rank and order data points as a vital part of making decisions. Multi-attribute ranking systems are a common tool used to make these data-driven decisions. Such systems often take the form of a table-based visualization in which users assign weights to the attributes representing the quantifiable importance of each attribute to a decision, which the system then uses to compute a ranking of the data. However, these systems assume that users are able to quantify their conceptual understanding of how important particular attributes are to a decision. This is not always easy or even possible for users to do. Rather, people often have a more holistic understanding of the data. They form opinions that data point A is better than data point B but do not necessarily know which attributes are important. To address these challenges, we present a visual analytic application to help people rank multi-variate data points. We developed a prototype system, Podium, that allows users to drag rows in the table to rank order data points based on their perception of the relative value of the data. Podium then infers a weighting model using Ranking SVM that satisfies the user's data preferences as closely as possible. Whereas past systems help users understand the relationships between data points based on changes to attribute weights, our approach helps users to understand the attributes that might inform their understanding of the data. We present two usage scenarios to describe some of the potential uses of our proposed technique: (1) understanding which attributes contribute to a user's subjective preferences for data, and (2) deconstructing attributes of importance for existing rankings. Our proposed approach makes powerful machine learning techniques more usable to those who may not have expertise in these areas.},
   author = {Emily Wall and Subhajit Das and Ravish Chawla and Bharath Kalidindi and Eli T. Brown and Alex Endert},
   doi = {10.1109/TVCG.2017.2745078},
   issn = {10772626},
   issue = {1},
   journal = {IEEE Transactions on Visualization and Computer Graphics},
   title = {{Podium: Ranking Data Using Mixed-Initiative Visual Analytics}},
   volume = {24},
   year = {2018},
}
@ARTICLE{Xie2020,
  author={X. {Xie} and J. {Wang} and H. {Liang} and D. {Deng} and S. {Cheng} and H. {Zhang} and W. {Chen} and Y. {Wu}},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title = {{PassVizor: Toward Better Understanding of the Dynamics of Soccer Passes}}, 
  year={2020},
  volume={},
  number={},
  pages={1-1},
  abstract={In soccer, passing is the most frequent interaction between players and plays a significant role in creating scoring chances. Experts are interested in analyzing players' passing behavior to learn passing tactics, i.e., how players build up an attack with passing. Various approaches have been proposed to facilitate the analysis of passing tactics. However, the dynamic changes of a team's employed tactics over a match have not been comprehensively investigated. To address the problem, we closely collaborate with domain experts and characterize requirements to analyze the dynamic changes of a team's passing tactics. To characterize the passing tactic employed for each attack, we propose a topic-based approach that provides a high-level abstraction of complex passing behaviors. Based on the model, we propose a glyph-based design to reveal the multi-variate information of passing tactics within different phases of attacks, including player identity, spatial context, and formation. We further design and develop PassVizor, a visual analytics system, to support the comprehensive analysis of passing dynamics. With the system, users can detect the changing patterns of passing tactics and examine the detailed passing process for evaluating passing tactics. We invite experts to conduct analysis with PassVizor and demonstrate the usability of the system through an expert interview.},
  keywords={Sports;Visual analytics;Data visualization;Interviews;Pattern matching;Navigation;Soccer Analysis;Passing Analysis},
  doi={10.1109/TVCG.2020.3030359},
  ISSN={1941-0506},
  month={},}

@article{Hazarika2020,
   abstract = {Complex computational models are often designed to simulate real-world physical phenomena in many scientific disciplines. However, these simulation models tend to be computationally very expensive and involve a large number of simulation input parameters, which need to be analyzed and properly calibrated before the models can be applied for real scientific studies. We propose a visual analysis system to facilitate interactive exploratory analysis of high-dimensional input parameter space for a complex yeast cell polarization simulation. The proposed system can assist the computational biologists, who designed the simulation model, to visually calibrate the input parameters by modifying the parameter values and immediately visualizing the predicted simulation outcome without having the need to run the original expensive simulation for every instance. Our proposed visual analysis system is driven by a trained neural network-based surrogate model as the backend analysis framework. In this work, we demonstrate the advantage of using neural networks as surrogate models for visual analysis by incorporating some of the recent advances in the field of uncertainty quantification, interpretability and explainability of neural network-based models. We utilize the trained network to perform interactive parameter sensitivity analysis of the original simulation as well as recommend optimal parameter configurations using the activation maximization framework of neural networks. We also facilitate detail analysis of the trained network to extract useful insights about the simulation model, learned by the network, during the training process. We performed two case studies, and discovered multiple new parameter configurations, which can trigger high cell polarization results in the original simulation model. We evaluated our results by comparing with the original simulation model outcomes as well as the findings from previous parameter analysis performed by our experts.},
   author = {Subhashis Hazarika and Haoyu Li and Ko Chih Wang and Han Wei Shen and Ching Shan Chou},
   doi = {10.1109/TVCG.2019.2934591},
   issn = {19410506},
   issue = {1},
   journal = {IEEE Transactions on Visualization and Computer Graphics},
   title = {{NNVA: Neural Network Assisted Visual Analysis of Yeast Cell Polarization Simulation}},
   volume = {26},
   year = {2020},
}
@article{Cuenca2018,
   abstract = {Multiple time series are a set of multiple quantitative variables occurring at the same interval. They are present in many domains such as medicine, finance, and manufacturing for analytical purposes. In recent years, streamgraph visualization (evolved from ThemeRiver) has been widely used for representing temporal evolution patterns in multiple time series. However, streamgraph as well as ThemeRiver suffer from scalability problems when dealing with several time series. To solve this problem, multiple time series can be organized into a hierarchical structure where individual time series are grouped hierarchically according to their proximity. In this paper, we present a new streamgraph-based approach to convey the hierarchical structure of multiple time series to facilitate the exploration and comparisons of temporal evolution. Based on a focus+context technique, our method allows time series exploration at different granularities (e.g., from overview to details). To illustrate our approach, two usage examples are presented.},
   author = {Erick Cuenca and Arnaud Sallaberry and Florence Y. Wang and Pascal Poncelet},
   doi = {10.1109/TVCG.2018.2796591},
   issn = {19410506},
   issue = {12},
   journal = {IEEE Transactions on Visualization and Computer Graphics},
   title = {{MultiStream: A Multiresolution Streamgraph Approach to Explore Hierarchical Time Series}},
   volume = {24},
   year = {2018},
}
@article{Zhao2020,
   abstract = {Evaluating employee performance in organizations with varying workloads and tasks is challenging. Specifically, it is important to understand how quantitative measurements of employee achievements relate to supervisor expectations, what the main drivers of good performance are, and how to combine these complex and flexible performance evaluation metrics into an accurate portrayal of organizational performance in order to identify shortcomings and improve overall productivity. To facilitate this process, we summarize common organizational performance analyses into four visual exploration task categories. Additionally, we develop MetricsVis, a visual analytics system composed of multiple coordinated views to support the dynamic evaluation and comparison of individual, team, and organizational performance in public safety organizations. MetricsVis provides four primary visual components to expedite performance evaluation: (1) a priority adjustment view to support direct manipulation on evaluation metrics; (2) a reorderable performance matrix to demonstrate the details of individual employees; (3) a group performance view that highlights aggregate performance and individual contributions for each group; and (4) a projection view illustrating employees with similar specialties to facilitate shift assignments and training. We demonstrate the usability of our framework with two case studies from medium-sized law enforcement agencies and highlight its broader applicability to other domains.},
   author = {Jieqiong Zhao and Morteza Karimzadeh and Luke S. Snyder and Chittayong Surakitbanharn and Zhenyu Cheryl Qian and David S. Ebert},
   doi = {10.1109/TVCG.2019.2934603},
   issn = {19410506},
   issue = {1},
   journal = {IEEE Transactions on Visualization and Computer Graphics},
   title = {{MetricsVis: A Visual Analytics System for Evaluating Employee Performance in Public Safety Agencies}},
   volume = {26},
   year = {2020},
}
@article{Wu2018,
   abstract = {The rapid development of information technology paved the way for the recording of fine-grained data, such as stroke techniques and stroke placements, during a table tennis match. This data recording creates opportunities to analyze and evaluate matches from new perspectives. Nevertheless, the increasingly complex data poses a significant challenge to make sense of and gain insights into. Analysts usually employ tedious and cumbersome methods which are limited to watching videos and reading statistical tables. However, existing sports visualization methods cannot be applied to visualizing table tennis competitions due to different competition rules and particular data attributes. In this work, we collaborate with data analysts to understand and characterize the sophisticated domain problem of analysis of table tennis data. We propose iTTVis, a novel interactive table tennis visualization system, which to our knowledge, is the first visual analysis system for analyzing and exploring table tennis data. iTTVis provides a holistic visualization of an entire match from three main perspectives, namely, time-oriented, statistical, and tactical analyses. The proposed system with several well-coordinated views not only supports correlation identification through statistics and pattern detection of tactics with a score timeline but also allows cross analysis to gain insights. Data analysts have obtained several new insights by using iTTVis. The effectiveness and usability of the proposed system are demonstrated with four case studies.},
   author = {Yingcai Wu and Ji Lan and Xinhuan Shu and Chenyang Ji and Kejian Zhao and Jiachen Wang and Hui Zhang},
   doi = {10.1109/TVCG.2017.2744218},
   issn = {10772626},
   issue = {1},
   journal = {IEEE Transactions on Visualization and Computer Graphics},
   title = {{ITTVis: Interactive Visualization of Table Tennis Data}},
   volume = {24},
   year = {2018},
}
@article{Behrisch2020,
   abstract = {Matrix representations are one of the main established and empirically proven to be effective visualization techniques for relational (or network) data. However, matrices-similar to node-link diagrams-are most effective if their layout reveals the underlying data topology. Given the many developed algorithms, a practical problem arises: 'Which matrix reordering algorithm should I choose for my dataset at hand?' To make matters worse, different reordering algorithms applied to the same dataset may let significantly different visual matrix patterns emerge. This leads to the question of trustworthiness and explainability of these fully automated, often heuristic, black-box processes. We present GUIRO, a Visual Analytics system that helps novices, network analysts, and algorithm designers to open the black-box. Users can investigate the usefulness and expressiveness of 70 accessible matrix reordering algorithms. For network analysts, we introduce a novel model space representation and two interaction techniques for a user-guided reordering of rows or columns, and especially groups thereof (submatrix reordering). These novel techniques contribute to the understanding of the global and local dataset topology. We support algorithm designers by giving them access to 16 reordering quality metrics and visual exploration means for comparing reordering implementations on a row/column permutation level. We evaluated GUIRO in a guided explorative user study with 12 subjects, a case study demonstrating its usefulness in a real-world scenario, and through an expert study gathering feedback on our design decisions. We found that our proposed methods help even inexperienced users to understand matrix patterns and allow a user-guided steering of reordering algorithms. GUIRO helps to increase the transparency of matrix reordering algorithms, thus helping a broad range of users to get a better insight into the complex reordering process, in turn supporting data and reordering algorithm insights.},
   author = {Michael Behrisch and Tobias Schreck and Hanspeter Pfister},
   doi = {10.1109/TVCG.2019.2934300},
   issn = {19410506},
   issue = {1},
   journal = {IEEE Transactions on Visualization and Computer Graphics},
   title = {{GUIRO: User-Guided Matrix Reordering}},
   volume = {26},
   year = {2020},
}
@article{Wu2019,
   abstract = {Regarded as a high-level tactic in soccer, a team formation assigns players different tasks and indicates their active regions on the pitch, thereby influencing the team performance significantly. Analysis of formations in soccer has become particularly indispensable for soccer analysts. However, formations of a team are intrinsically time-varying and contain inherent spatial information. The spatio-temporal nature of formations and other characteristics of soccer data, such as multivariate features, make analysis of formations in soccer a challenging problem. In this study, we closely worked with domain experts to characterize domain problems of formation analysis in soccer and formulated several design goals. We design a novel spatio-temporal visual representation of changes in team formation, allowing analysts to visually analyze the evolution of formations and track the spatial flow of players within formations over time. Based on the new design, we further design and develop ForVizor, a visual analytics system, which empowers users to track the spatio-temporal changes in formation and understand how and why such changes occur. With ForVizor, domain experts conduct formation analysis of two games. Analysis results with insights and useful feedback are summarized in two case studies.},
   author = {Yingcai Wu and Xiao Xie and Jiachen Wang and Dazhen Deng and Hongye Liang and Hui Zhang and Shoubin Cheng and Wei Chen},
   doi = {10.1109/TVCG.2018.2865041},
   issn = {19410506},
   issue = {1},
   journal = {IEEE Transactions on Visualization and Computer Graphics},
   title = {{ForVizor: Visualizing Spatio-Temporal Team Formations in Soccer}},
   volume = {25},
   year = {2019},
}
@article{Mumtaz2020,
   abstract = {Good code quality is a prerequisite for efficiently developing maintainable software. In this paper, we present a novel approach to generate exploranative (explanatory and exploratory) data-driven documents that report code quality in an interactive, exploratory environment. We employ a template-based natural language generation method to create textual explanations about the code quality, dependent on data from software metrics. The interactive document is enriched by different kinds of visualization, including parallel coordinates plots and scatterplots for data exploration and graphics embedded into text. We devise an interaction model that allows users to explore code quality with consistent linking between text and visualizations; through integrated explanatory text, users are taught background knowledge about code quality aspects. Our approach to interactive documents was developed in a design study process that included software engineering and visual analytics experts. Although the solution is specific to the software engineering scenario, we discuss how the concept could generalize to multivariate data and report lessons learned in a broader scope.},
   author = {Haris Mumtaz and Shahid Latif and Fabian Beck and Daniel Weiskopf},
   doi = {10.1109/TVCG.2019.2934669},
   issn = {19410506},
   issue = {1},
   journal = {IEEE Transactions on Visualization and Computer Graphics},
   title = {{Exploranative Code Quality Documents}},
   volume = {26},
   year = {2020},
}
@article{Law2019,
   abstract = {Data analysis novices often encounter barriers in executing low-level operations for pairwise comparisons. They may also run into barriers in interpreting the artifacts (e.g., visualizations) created as a result of the operations. We developed Duet, a visual analysis system designed to help data analysis novices conduct pairwise comparisons by addressing execution and interpretation barriers. To reduce the barriers in executing low-level operations during pairwise comparison, Duet employs minimal specification: when one object group (i.e. a group of records in a data table) is specified, Duet recommends object groups that are similar to or different from the specified one; when two object groups are specified, Duet recommends similar and different attributes between them. To lower the barriers in interpreting its recommendations, Duet explains the recommended groups and attributes using both visualizations and textual descriptions. We conducted a qualitative evaluation with eight participants to understand the effectiveness of Duet. The results suggest that minimal specification is easy to use and Duet's explanations are helpful for interpreting the recommendations despite some usability issues.},
   author = {Po Ming Law and Rahul C. Basole and Yanhong Wu},
   doi = {10.1109/TVCG.2018.2864526},
   issn = {19410506},
   issue = {1},
   journal = {IEEE Transactions on Visualization and Computer Graphics},
   title = {{Duet: Helping Data Analysis Novices Conduct Pairwise Comparisons by Minimal Specification}},
   volume = {25},
   year = {2019},
}
@article{Wang2019,
   abstract = {Deep Q-Network (DQN), as one type of deep reinforcement learning model, targets to train an intelligent agent that acquires optimal actions while interacting with an environment. The model is well known for its ability to surpass professional human players across many Atari 2600 games. Despite the superhuman performance, in-depth understanding of the model and interpreting the sophisticated behaviors of the DQN agent remain to be challenging tasks, due to the long-time model training process and the large number of experiences dynamically generated by the agent. In this work, we propose DQNViz, a visual analytics system to expose details of the blind training process in four levels, and enable users to dive into the large experience space of the agent for comprehensive analysis. As an initial attempt in visualizing DQN models, our work focuses more on Atari games with a simple action space, most notably the Breakout game. From our visual analytics of the agent's experiences, we extract useful action/reward patterns that help to interpret the model and control the training. Through multiple case studies conducted together with deep learning experts, we demonstrate that DQNViz can effectively help domain experts to understand, diagnose, and potentially improve DQN models.},
   author = {Junpeng Wang and Liang Gou and Han Wei Shen and Hao Yang},
   doi = {10.1109/TVCG.2018.2864504},
   issn = {19410506},
   issue = {1},
   journal = {IEEE Transactions on Visualization and Computer Graphics},
   title = {{DQNViz: A Visual Analytics Approach to Understand Deep Q-Networks}},
   volume = {25},
   year = {2019},
}
@article{Xu2020,
   abstract = {Detecting and analyzing potential anomalous performances in cloud computing systems is essential for avoiding losses to customers and ensuring the efficient operation of the systems. To this end, a variety of automated techniques have been developed to identify anomalies in cloud computing. These techniques are usually adopted to track the performance metrics of the system (e.g., CPU, memory, and disk I/O), represented by a multivariate time series. However, given the complex characteristics of cloud computing data, the effectiveness of these automated methods is affected. Thus, substantial human judgment on the automated analysis results is required for anomaly interpretation. In this paper, we present a unified visual analytics system named CloudDet to interactively detect, inspect, and diagnose anomalies in cloud computing systems. A novel unsupervised anomaly detection algorithm is developed to identify anomalies based on the specific temporal patterns of the given metrics data (e.g., the periodic pattern). Rich visualization and interaction designs are used to help understand the anomalies in the spatial and temporal context. We demonstrate the effectiveness of CloudDet through a quantitative evaluation, two case studies with real-world data, and interviews with domain experts.},
   author = {Ke Xu and Yun Wang and Leni Yang and Yifang Wang and Bo Qiao and Si Qin and Yong Xu and Haidong Zhang and Huamin Qu},
   doi = {10.1109/TVCG.2019.2934613},
   issn = {19410506},
   issue = {1},
   journal = {IEEE Transactions on Visualization and Computer Graphics},
   title = {{CloudDet: Interactive Visual Analysis of Anomalous Performances in Cloud Computing Systems}},
   volume = {26},
   year = {2020},
}
@article{Deng2020,
   abstract = {Air pollution has become a serious public health problem for many cities around the world. To find the causes of air pollution, the propagation processes of air pollutants must be studied at a large spatial scale. However, the complex and dynamic wind fields lead to highly uncertain pollutant transportation. The state-of-The-Art data mining approaches cannot fully support the extensive analysis of such uncertain spatiotemporal propagation processes across multiple districts without the integration of domain knowledge. The limitation of these automated approaches motivates us to design and develop AirVis, a novel visual analytics system that assists domain experts in efficiently capturing and interpreting the uncertain propagation patterns of air pollution based on graph visualizations. Designing such a system poses three challenges: A) the extraction of propagation patterns; b) the scalability of pattern presentations; and c) the analysis of propagation processes. To address these challenges, we develop a novel pattern mining framework to model pollutant transportation and extract frequent propagation patterns efficiently from large-scale atmospheric data. Furthermore, we organize the extracted patterns hierarchically based on the minimum description length (MDL) principle and empower expert users to explore and analyze these patterns effectively on the basis of pattern topologies. We demonstrated the effectiveness of our approach through two case studies conducted with a real-world dataset and positive feedback from domain experts.},
   author = {Zikun Deng and Di Weng and Jiahui Chen and Ren Liu and Zhibin Wang and Jie Bao and Yu Zheng and Yingcai Wu},
   doi = {10.1109/TVCG.2019.2934670},
   issn = {19410506},
   issue = {1},
   journal = {IEEE Transactions on Visualization and Computer Graphics},
   title = {{AirVis: Visual Analytics of Air Pollution Propagation}},
   volume = {26},
   year = {2020},
}
@article{Xie2019,
   abstract = {Anomalous runtime behavior detection is one of the most important tasks for performance diagnosis in High Performance Computing (HPC). Most of the existing methods find anomalous executions based on the properties of individual functions, such as execution time. However, it is insufficient to identify abnormal behavior without taking into account the context of the executions, such as the invocations of children functions and the communications with other HPC nodes. We improve upon the existing anomaly detection approaches by utilizing the call stack structures of the executions, which record rich temporal and contextual information. With our call stack tree (CSTree) representation of the executions, we formulate the anomaly detection problem as finding anomalous tree structures in a call stack forest. The CSTrees are converted to vector representations using our proposed stack2vec embedding. Structural and temporal visualizations of CSTrees are provided to support users in the identification and verification of the anomalies during an active anomaly detection process. Three case studies of real-world HPC applications demonstrate the capabilities of our approach.},
   author = {Cong Xie and Wei Xu and Klaus Mueller},
   doi = {10.1109/TVCG.2018.2865026},
   issn = {19410506},
   issue = {1},
   journal = {IEEE Transactions on Visualization and Computer Graphics},
   title = {{A Visual Analytics Framework for the Detection of Anomalous Call Stack Trees in High Performance Computing Applications}},
   volume = {25},
   year = {2019},
}
@article{Xie2017,
   abstract = {Oftentimes multivariate data are not available as sets of equally multivariate tuples, but only as sets of projections into subspaces spanned by subsets of these attributes. For example, one may find data with five attributes stored in six tables of two attributes each, instead of a single table of five attributes. This prohibits the visualization of these data with standard high-dimensional methods, such as parallel coordinates or MDS, and there is hence the need to reconstruct the full multivariate (joint) distribution from these marginal ones. Most of the existing methods designed for this purpose use an iterative procedure to estimate the joint distribution. With insufficient marginal distributions and domain knowledge, they lead to results whose joint errors can be large. Moreover, enforcing smoothness for regularizations in the joint space is not applicable if the attributes are not numerical but categorical. We propose a visual analytics approach that integrates both anecdotal data and human experts to iteratively narrow down a large set of plausible solutions. The solution space is populated using a Monte Carlo procedure which uniformly samples the solution space. A level-of-detail high dimensional visualization system helps the user understand the patterns and the uncertainties. Constraints that narrow the solution space can then be added by the user interactively during the iterative exploration, and eventually a subset of solutions with narrow uncertainty intervals emerges.},
   author = {Cong Xie and Wen Zhong and Klaus Mueller},
   doi = {10.1109/TVCG.2016.2598479},
   issn = {10772626},
   issue = {1},
   journal = {IEEE Transactions on Visualization and Computer Graphics},
   title = {{A Visual Analytics Approach for Categorical Joint Distribution Reconstruction from Marginal Projections}},
   volume = {23},
   year = {2017},
}
@article{Guo2016,
   abstract = {We present results from an experiment aimed at using logs of interactions with a visual analytics application to better understand how interactions lead to insight generation. We performed an insight-based user study of a visual analytics application and ran post hoc quantitative analyses of participants' measured insight metrics and interaction logs. The quantitative analyses identified features of interaction that were correlated with insight characteristics, and we confirmed these findings using a qualitative analysis of video captured during the user study. Results of the experiment include design guidelines for the visual analytics application aimed at supporting insight generation. Furthermore, we demonstrated an analysis method using interaction logs that identified which interaction patterns led to insights, going beyond insight-based evaluations that only quantify insight characteristics. We also discuss choices and pitfalls encountered when applying this analysis method, such as the benefits and costs of applying an abstraction framework to application-specific actions before further analysis. Our method can be applied to evaluations of other visualization tools to inform the design of insight-promoting interactions and to better understand analyst behaviors.},
   author = {Hua Guo and Steven R. Gomez and Caroline Ziemkiewicz and David H. Laidlaw},
   doi = {10.1109/TVCG.2015.2467613},
   issn = {10772626},
   issue = {1},
   journal = {IEEE Transactions on Visualization and Computer Graphics},
   title = {{A Case Study Using Visualization Interaction Logs and Insight Metrics to Understand How Analysts Arrive at Insights}},
   volume = {22},
   year = {2016},
}
@article{Yu2017,
   abstract = {Data flow systems allow the user to design a flow diagram that specifies the relations between system components which process, filter or visually present the data. Visualization systems may benefit from user-defined data flows as an analysis typically consists of rendering multiple plots on demand and performing different types of interactive queries across coordinated views. In this paper, we propose VisFlow, a web-based visualization framework for tabular data that employs a specific type of data flow model called the subset flow model. VisFlow focuses on interactive queries within the data flow, overcoming the limitation of interactivity from past computational data flow systems. In particular, VisFlow applies embedded visualizations and supports interactive selections, brushing and linking within a visualization-oriented data flow. The model requires all data transmitted by the flow to be a data item subset (i.e. groups of table rows) of some original input table, so that rendering properties can be assigned to the subset unambiguously for tracking and comparison. VisFlow features the analysis flexibility of a flow diagram, and at the same time reduces the diagram complexity and improves usability. We demonstrate the capability of VisFlow on two case studies with domain experts on real-world datasets showing that VisFlow is capable of accomplishing a considerable set of visualization and analysis tasks. The VisFlow system is available as open source on GitHub.},
   author = {Bowen Yu and Cláudio T. Silva},
   doi = {10.1109/TVCG.2016.2598497},
   issn = {10772626},
   issue = {1},
   journal = {IEEE Transactions on Visualization and Computer Graphics},
   title = {{VisFlow - Web-based Visualization Framework for Tabular Data with a Subset Flow Model}},
   volume = {23},
   year = {2017},
}
@article{Satyanarayan2017,
   abstract = {We present Vega-Lite, a high-level grammar that enables rapid specification of interactive data visualizations. Vega-Lite combines a traditional grammar of graphics, providing visual encoding rules and a composition algebra for layered and multi-view displays, with a novel grammar of interaction. Users specify interactive semantics by composing selections. In Vega-Lite, a selection is an abstraction that defines input event processing, points of interest, and a predicate function for inclusion testing. Selections parameterize visual encodings by serving as input data, defining scale extents, or by driving conditional logic. The Vega-Lite compiler automatically synthesizes requisite data flow and event handling logic, which users can override for further customization. In contrast to existing reactive specifications, Vega-Lite selections decompose an interaction design into concise, enumerable semantic units. We evaluate Vega-Lite through a range of examples, demonstrating succinct specification of both customized interaction methods and common techniques such as panning, zooming, and linked selection.},
   author = {Arvind Satyanarayan and Dominik Moritz and Kanit Wongsuphasawat and Jeffrey Heer},
   doi = {10.1109/TVCG.2016.2599030},
   issn = {10772626},
   issue = {1},
   journal = {IEEE Transactions on Visualization and Computer Graphics},
   title = {{Vega-Lite: A Grammar of Interactive Graphics}},
   volume = {23},
   year = {2017},
}

@Misc{WEB:tableau,
  title = {{Tableau}},
  howpublished = {\url{https://www.tableau.com/}},
  note         = {March, 2020},
  doi          = {10.1007/springerreference_6234},
  file         = {:WEB_tableau - Tableau.PDF:PDF},
}
@article{Yu2020,
   abstract = {Dataflow visualization systems enable flexible visual data exploration by allowing the user to construct a dataflow diagram that composes query and visualization modules to specify system functionality. However learning dataflow diagram usage presents overhead that often discourages the user. In this work we design FlowSense, a natural language interface for dataflow visualization systems that utilizes state-of-The-Art natural language processing techniques to assist dataflow diagram construction. FlowSense employs a semantic parser with special utterance tagging and special utterance placeholders to generalize to different datasets and dataflow diagrams. It explicitly presents recognized dataset and diagram special utterances to the user for dataflow context awareness. With FlowSense the user can expand and adjust dataflow diagrams more conveniently via plain English. We apply FlowSense to the VisFlow subset-flow visualization system to enhance its usability. We evaluate FlowSense by one case study with domain experts on a real-world data analysis problem and a formal user study.},
   author = {Bowen Yu and Claudio T. Silva},
   doi = {10.1109/TVCG.2019.2934668},
   issn = {19410506},
   issue = {1},
   journal = {IEEE Transactions on Visualization and Computer Graphics},
   title = {{FlowSense: A Natural Language Interface for Visual Data Exploration within a Dataflow System}},
   volume = {26},
   year = {2020},
}
@inproceedings{North2002,
   abstract = {Relational databases provide significant flexibility to organize, store, and manipulate an infinite variety of complex data collections. This flexibility is enabled by the concept of relational data schemas, which allow data owners to easily design custom databases according to their unique needs. However, user interfaces and information visualizations for accessing and utilizing databases have not kept pace with this level of flexibility. This paper introduces the concept of visualization schemas, based on the Snap-Together Visualization model, which are analogous to relational data schemas. Visualization schemas enable users to rapidly construct customized multiple-view visualizations for databases in a similarly flexible fashion without programming. Since the design of appropriate visualizations for a given database depends on the data schema, visualization schemas are a natural analogy to the data schema concept.},
   author = {C. North and N. Conklin and V. Saini},
   doi = {10.1109/INFVIS.2002.1173142},
   issn = {1522404X},
   journal = {Proceedings - IEEE Symposium on Information Visualization, INFO VIS},
   title = {{Visualization Schemas for Flexible Information Visualization}},
   volume = {2002-January},
   year = {2002},
}
@article{Pattison2001,
   abstract = {A view is a particular visual representation of a data set. Com- plex data sets typically require multiple views, each revealing a dierent aspect of the data. Coordinating the behaviour of these views is known to expedite tasks such as information seeking, and has been used to facilitate the exploration of large and complex data sets. The design and implementation of mul- tiple view coordinations is challenging; poorly designed coordi- nations may in fact detract from task performance, while the failure to make coordinations apparent could lead to unnec- cessarily complex mental models of a data set. In order to facilitate the exploration of this design space, we present here an architecture for the implementation of view coordinations. In contrast with many existing tools and prototypes, which fo- cus on specific view coordination techniques, this architecture provides generic support for view coordination. The implemen- tation of several standard coordination techniques within the architecture are discussed by way of illustration, and the ar- chitecture compared with the Snap view coordination architec- ture. Examples of its initial implementation within the InVi- sion component-based framework for information visualisation are also presented.},
   author = {Tim Pattison and Matthew Phillips},
   journal = {Proceedings of the 2001 Asia-Pacific symposium on Information visualisation-Volume 9},
   title = {{View Coordination Architecture for Information Visualisation}},
   year = {2001},
}
@inproceedings{Roberts2007,
   abstract = {The area of Coordinated and Multiple Views has been steadily developing and maturing over the past fifteen years. Some may say that it is a 'solved problem', while others argue that we are only just scratching the surface of the subject. Considering merely the CMV conference series, it is clear to see that in the early years researchers were concerned with models and techniques, while in latter years authors presented more work on how to apply these ideas to different domains. It is our view that there is still much research to be done, but the subject is changing and developing as a tool for Visual Analytics. This paper provides the 'state of the art' of CMV, it describes areas that should be developed further and looks at what the future may hold for Coordinated and Multiple Views. © 2007 IEEE.},
   author = {Jonathan C. Roberts},
   doi = {10.1109/CMV.2007.20},
   journal = {Proceedings - Fifth International Conference on Coordinated and Multiple Views in Exploratory Visualization, CMV 2007},
   title = {{State of the Art: Coordinated & Multiple Views in Exploratory Visualization}},
   year = {2007},
}
@article{North2000,
   abstract = {Multiple coordinated visualizations enable users to rapidly explore complex information. However, users often need unforeseen combinations of coordinated visualizations. Snap-together visualization (Snap) enables users to rapidly and dynamically construct coordinated-visualization interfaces, customized for their data, without programming. Users load data into desired visualizations, then construct coordinations between them for brushing and linking, overview and detail view, drill down, etc. Snap formalizes a conceptual model of visualization coordination based on the relational data model. Visualization developers can easily Snap-enable their independent visualizations using a simple API. Empirical evaluation reveals benefits, cognitive issues and usability concerns with coordination concepts and Snap. Two user studies explore coordination construction and operation. Data-savvy users successfully, enthusiastically and rapidly constructed powerful coordinated-visualization interfaces of their own. Operating an overview-and-detail coordination reliably improved user performance by 30-80% over detail-only and uncoordinated interfaces for most tasks.},
   author = {Chris North and Ben Shneiderman},
   doi = {10.1006/ijhc.2000.0418},
   issn = {10715819},
   issue = {5},
   journal = {International Journal of Human Computer Studies},
   title = {{Snap-Together Visualization: Can Users Construct and Operate Coordinated Visualizations?}},
   volume = {53},
   year = {2000},
}
@inproceedings{Roberts2000,
   abstract = {Visualization is about discovery and understanding; the user wishes to gain a correct insight into the underlying information, to explore and analyze how different parts are related. Thus, presentation, exploration and explanation tools are used with manipulation and investigative techniques to display, discover and gain a 'correct dissemination' of the information. Moreover, by displaying the information simultaneously in multiple ways the user is aided in their investigation. Such multiform techniques may be generated through various algorithms; we organize these methods according to how they apply to the individual stages of the dataflow paradigm. These multiforms may be displayed in separate windows. Multiple views are useful (1) to overcome misinterpretations and provide additional insight, (2) for scientific exploration tasks of relating, coupling and to aid the 'drilling down' of information, and (3) to provide alter-native viewpoints by expressing different user-interpretations of the same information. Finally, to use multiple views effectively they should be, among other things, easily created, automatically coupled to other views and dynamically manipulated.},
   author = {Jonathan C Roberts},
   doi = {10.1117/12.378894},
   editor = {Robert F Erbacher and Philip C Chen and Jonathan C Roberts and Craig M Wittenbrink},
   journal = {Visual Data Exploration and Analysis VII},
   pages = {176 – 185},
   publisher = {SPIE},
   title = {{Multiple View and Multiform Visualization}},
   volume = {3960},
   url = {https://doi.org/10.1117/12.378894},
   year = {2000},
}
@article{Baldonado2000,
   abstract = {A multiple view system uses two or more distinct views to support the investigation of a single conceptual entity. Many such systems exist, ranging from computer-aided design (CAD) systems for chip design that display both the logical structure and the actual geometry of the integrated circuit to overview-plus-detail systems that show both an overview for context and a zoomed-in-view for detail. Designers of these systems must make a variety of design decisions, ranging from determining layout to constructing sophisticated coordination mechanisms. Surprisingly, little work has been done to characterize these systems or to express guidelines for their design. Based on a workshop discussion of multiple views, and based on our own design and implementation experience with these systems, we present eight guidelines for the design of multiple view systems.},
   author = {Michelle Q.Wang Baldonado and Allison Woodruff and Allan Kuchinsky},
   doi = {10.1145/345513.345271},
   journal = {Proceedings of the Workshop on Advanced Visual Interfaces},
   title = {{Guidelines for Using Multiple Views in Information Visualization}},
   year = {2000},
}
@inproceedings{Andrienko2007,
   author = {Gennady Andrienko and Natalia Andrienko},
   doi = {10.1109/CMV.2007.4},
   journal = {Proceedings - Fifth International Conference on Coordinated and Multiple Views in Exploratory Visualization, CMV 2007},
   title = {{Cordinated Multiple Views: A Critical View}},
   year = {2007},
}
@inproceedings{Weaver2004,
   abstract = {Improvise is a fully-implemented system in which users build and browse multiview visualizations interactively using a simple shared-object coordination mechanism coupled with a flexible, expression-based visual abstraction language. By coupling visual abstraction with coordination, users gain precise control over how navigation and selection in a visualization affects the appearance of data in individual views. As a result, it is practical to build visualizations with more views and richer coordination in Improvise than in other visualization systems. Building and browsing activities are integrated in a single, live user interface that lets users alter visualizations quickly and incrementally during data exploration. ©2004 IEEE.},
   author = {Chris Weaver},
   doi = {10.1109/infvis.2004.12},
   issn = {1522404X},
   journal = {Proceedings - IEEE Symposium on Information Visualization, INFO VIS},
   title = {{Building Highly-Coordinated Visualizations in Improvise}},
   year = {2004},
}
@inproceedings{Boukhelifa2003,
   abstract = {We present a coordination model for exploratory multiview visualization. We base our work on current research in exploratory visualization and other disciplines. Our model is based on sharing abstract objects such as the visualization parameters of the dataflow model to achieve coordinated exploratory tasks in multiple views. This model describes how current coordinations in exploratory visualization work and allows novel coordinations to be constructed.},
   author = {N Boukhelifa and J C Roberts and P J Rodgers},
   doi = {10.1109/CMV.2003.1215005},
   journal = {Proceedings International Conference on Coordinated and Multiple Views in Exploratory Visualization - CMV 2003 -},
   keywords = {data visualisation;graphical user interfaces;data flow computing;data models;coordination model;exploratory multiview visualization;dataflow model;reference model;Data visualization;Information processing;History;Protocols;Ontologies;Laboratories;User interfaces;Displays;TV;Project management},
   month = {7},
   pages = {76-85},
   title = {{A Coordination Model for Exploratory Multiview Visualization}},
   year = {2003},
}
@article{North1997,
   abstract = {In current windowing environments, individual windows are treated independently, making it difficult for users to coordinate information across multiple windows. While coordinated multi-window strategies are increasingly used in visualization and web user interfaces, designs are inflexible and haphazard. The space of such linked- window strategies is not well understood and largely unexplored. This paper presents a taxonomy of coordinations, identifies important components, and reviews example interfaces. This 2x3 taxonomy provides guidelines for designers of applications, user interface toolkits, and window managers. We hope to encourage construction of generalized, end-user programmable, robust, multiple-window coordination capabilities.},
   author = {C. North and Ben Shneiderman},
   journal = {Engineering},
   title = {{A Taxonomy of Multiple Window Coordination}},
   year = {1997},
}

@Comment{jabref-meta: databaseType:bibtex;}
